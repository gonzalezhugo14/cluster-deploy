---
# Log level 0 - debug
# k8s_log_level: 3
# Use to increase verbosity on particular files, e.g. k8s_log_spec: 'token_controller*=5,other_controller*=4'
# k8s_log_spec: ''

# Kubernetes version
# k8s_version: 1.7.6

# Helm package manager version
# helm_version: 2.6.1

# Golang compiller version
# go_version: 1.9

# Account name of remote user. Ansible will use this user account to ssh into
# the managed machines. The user must be able to use sudo without asking
# for password
k8s_ssh_user: dev

# Account name of remote user. Ansible will use this user account to ssh into
# the build machines. The user must be able to use sudo without asking
# for password for build components
k8s_build_ssh_user: dev

# List of IPs which allowed to connect via ssh by `k8s_ssh_user`
k8s_ssh_allowed_ips:

# SSH key that will be used to connect your VM instances
# Use this default value if you won't use special separate key for this purposes
# k8s_ssh_key: '{{ lookup("file", "{{ ansible_env.HOME }}/.ssh/id_rsa.pub") }}'

# It will be used as the Internal dns domain name if DNS is enabled.
# Services will be discoverable under
# <service-name>.<namespace>.svc.<domainname>.<clustername>, e.g.
# myservice.default.svc.k8s.cluster
k8s_domain_name: k8s
k8s_cluster_name: cluster
k8s_cluster_domain: '{{ k8s_domain_name }}.{{ k8s_cluster_name }}'

# Additional Kubernetes namespaces
k8s_namespaces:
  - dev

# On-prem LB services
# List of services which use TCP LB for k8s masters/nodes
# master_services: {}
# node_services: {}
# gateway_services: {}

# Example
# node_services:
#   - name: whoisd
#     port: 43
#     nodePort: 30043

master_services:
  - name: master-80
    port: 80
    httpRedirect: true
  - name: master-443
    port: 443
    nodePort: 443
    httpCheck:
      path: /healthz
      status: 401
    sslProxy: true

node_services:
  - name: node-80
    port: 80
    nodePort: 80
    httpCheck:
      path: /healthz
      status: 200
  - name: node-443
    port: 443
    nodePort: 443
    httpCheck:
      path: /healthz
      status: 200
    sslProxy: true

gateway_services:
  - name: istio-80
    port: 80
    nodePort: 30080
  - name: istio-443
    port: 443
    nodePort: 30443
    sslProxy: true

# External services which can be outside of k8s cluster
# external_services: {}
#
# Example
# external_services:
#   - name: elastic
#     host: elastic.your-domain-name
#     port: 9200

# Path to the Inventory hosts file
# It should be auto generated during the crating of VM inctances
k8s_inventory_file: '{{ inventory_dir }}/{{ k8s_cluster_name }}'

# Kubernetes master and services host names
k8s_master_name: master.your-domain-name
k8s_services_name: services.your-domain-name

# Docker registry host name
k8s_registry_name: registry.your-domain-name

# Prometheus host names
k8s_prometheus_name: prometheus.your-domain-name
k8s_prometheus_alertmanager_name: alertmanager.your-domain-name

# Charts server repository
# k8s_charts_repo: github.com/k8s-community/charts

# Country name which used in `C` attribute of certificates (`NL`,`RU`, etc)
ssl_country: country-name

# City name which used in `L` attribute of certificates
ssl_city: city-name

# Organization name which used in `O` attribute of certificates
ssl_org: organization-name

# Organization Unit name which used in `OU` attribute of certificates
ssl_division: organization-unit-name

# State name which used in `ST` attribute of certificates
ssl_state: state-name

# SSL base certificate name
# SSL folder and file names will use the same name
ssl_name: kubernetes

# Path to files with SSL certificates and keys
ssl_dir: /etc/ssl/kubernetes

# Kubernetes service API port and health check port
# k8s_api_port: 443
# k8s_api_health_check_port: 8080

# User services port and health check port
# k8s_services_port: 443
# k8s_health_check_port: 80

# Flannel backend type (Options: gce, vxlan)
# k8s_flannel_backend: vxlan

# Kubernetes internal network for services.
# Kubernetes services will get fake IP addresses from this range.
# This range must not conflict with anything in your infrastructure. These
# addresses do not need to be routable and must just be an unused block of space.
k8s_services_network: 10.254.0.0/16

# IP address of kubernetes service
# Usually it's first address in services subnet
k8s_cluster_service_ip: 10.254.0.1

# IP address of Kube DNS
# It should be in range of services subnet
k8s_cluster_dns: 10.254.0.10

# Internal overlay network. It will assign IP
# addresses from this range to individual pods.
# This network must be unused block of space.
# k8s_cluster_cidr: 10.20.0.0/16

# It should be an IP addresses range corresponed with zone, e.g.
# Western Europe Zone has 10.132.0.0/20 IP range
# Use default IP range for determined zone in most of all cases
# gce_instances_ip_range: 10.132.0.0/24

# The network determines what network traffic the instance can access
# Use auto generated network name in most of all cases
# k8s_network_name: '{{ k8s_domain_name }}'

# Assigns the instance an IPv4 address from the subnetworkâ€™s range.
# Use auto generated subnetwork name in most of all cases
# gce_subnet_name: '{{ gce_network_name }}-{{ k8s_cluster_name }}'

# Forwarding allows the instance to help route packets
# gce_ip_forward: true

# A region is a specific geographical location where you can run your resources.
# Each region has one or more zones.
# gce_instances_region: europe-west1

# A zone is an isolated location within a region.
# Resources that live in a zone, such as instances,
# are referred to as zonal resources
# gce_instances_zone: europe-west1-b

# Conteiner Network Interface type, valid values: `calico`, `romana`
# cni_type: calico

# Using of network storage
# If network storage disabled will use local disk for every requested claim
# network_storage: false

# Kubernetes network persistent disk type, valid values: `gce`
# TODO: AWS persistent disk `aws`
# network_storage_type: gce

# Size of network persistent disk in Gb
# network_storage_size: 100

# Name of GCE persistent disk
# gce_storage_name: pd-std

# Type of GCE storage, options: `slow`, `fast`
# gce_storage_type: slow

# Ceph monitor port
# ceph_monitor_port: 6789

# Ceph pool name
# ceph_pull_name: rbd

# Ceph RBD image name
# ceph_rbd_image_name: rbdstore

# Ceph user name
# ceph_user_name: admin

# Ceph user token
# ceph auth print-key client.admin | base64
ceph_user_token: 'ceph user token here'

# Kubernetes load balancer type, valid values: `nginx`
# TODO: load balancer `gce`
k8s_lb_type: nginx

# PROXY protocol for ingress
# https://www.haproxy.com/blog/haproxy/proxy-protocol/
# k8s_ingress_proxy_protocol: false

# List of groups with VM instance names and machine types
# Instance groups let you organize VM instances or use them
# in a load-balancing backend service
# Nodes contain comma separated list of instance names.
# Names must start with a lowercase letter followed by up to 63 lowercase letters,
# numbers, or hyphens, and cannot end with a hyphen
# Predefined machine types are managed by Google Compute Engine:
# https://cloud.google.com/compute/docs/machine-types
gce_groups:
  - name: master
    type: n1-standard-1
    nodes:
      - k8s-master-01
      - k8s-master-02
  - name: node
    type: n1-standard-2
    nodes:
      - k8s-node-01
      - k8s-node-02
  - name: build
    type: n1-standard-2
    nodes:
      - k8s-build-01

# Each instance requires a disk to boot from.
# Specify an image when you create an instance.
# List of predefined images you can find in this reference
# https://cloud.google.com/compute/docs/images
gce_image: centos-7

# You can choose to specify a startup script that will run when your instance
# boots up or restarts. Startup scripts can be used to install software and updates,
# and to ensure that services are running within the virtual machine.
# Learn more: https://cloud.google.com/compute/docs/startupscript
gce_startup_script: |
  #!/bin/bash

  yum update -y

# Select the type and level of API access to grant the VM.
# Default: read-only access to Storage and Service Management,
# write access to Stackdriver Logging and Monitoring,
# read/write access to Service Control.
# gce_sa_permissions:
#   - compute-rw
#   - logging-write
#   - monitoring-write
#   - service-control
#   - service-management
#   - storage-full
#   - useraccounts-ro

# Users access data
k8s_admin_token: 'Admin user token should be here'
k8s_admin_username: admin
k8s_admin_password: 'password'
k8s_release_token: 'Release user token should be here'
k8s_release_username: release
k8s_release_password: 'password'
k8s_guest_token: 'Guest user token should be here'
k8s_guest_username: guest
k8s_guest_password: 'password'

# k8s.community services exchange token
k8s_community_token: 'k8s-community-token'

# k8s.community services databases credentials
k8s_community_db_username: 'k8s-community'
k8s_community_db_password: 'k8s.community'
k8s_github_integration_db_username: 'github-integration'
k8s_github_integration_db_password: 'github.integration'

# k8s.community Github integration services secrets
k8s_github_client_id: 'github client id here'
k8s_github_client_secret: 'github client secret here'
k8s_github_state: 'github state here'
k8s_github_integration_id: 'github integration id here'
k8s_github_integration_token: 'github integration token here'
k8s_github_integration_private_key: |
  -----BEGIN RSA PRIVATE KEY-----
  - Your RSA private key here -
  -----END RSA PRIVATE KEY-----

# SSL certificate and private key for k8s-community services
k8s_community_cert: |
  -----BEGIN CERTIFICATE-----
  - Your CERTIFICATE here -
  ------END CERTIFICATE------
k8s_community_key: |
  -----BEGIN RSA PRIVATE KEY-----
  - Your RSA private key here -
  -----END RSA PRIVATE KEY-----

# Docker registry secrets. To get it we should do some strange things, but it needs anyway.
# First of all, we should prepare access token for `Docker Registry`
# docker run --rm --entrypoint htpasswd registry:2 -Bbn <user> <password> | base64
k8s_docker_registry_token: 'docker registry token here'

# Second, we should create docker config with auth code, auth token and there are two ways:
#
# Solution 1:
# (without login to docker registry)
# kubectl create secret docker-registry my-secret --docker-username=user --docker-password='password' \
# --docker-email 'docker@docker.com' --docker-server=<docker_registry_host> --dry-run -o yaml
#
# grab hash in field `data.dockercfg` from output result of the command above
# echo '<hash from data.dockercfg>' | base64 --decode
#
# grab `auth code` from output result of the command above
#
# create `.docker/config.json`
# {
#   "auths": {
#     "<docker_registry_host>": {
#       "auth": "<auth_code_from_previous_command>"
#     }
#   }
# }
#
# Solution 2:
# (need real login to docker registry)
# docker login -u=<user> -p=<password> <docker_registry_host:port>

# Enter auth code from `.docker/config.json` here
k8s_docker_registry_auth_code: 'docker registry auth code here'

# Enter result of `cat .docker/config.json | base64` here
k8s_docker_registry_auth_token: 'docker registry auth config token'

# SSL certificate and private key for running user services into Kubernetes
k8s_services_cert: |
  ----BEGIN CERTIFICATE----
  - Your certificate here -
  -----END CERTIFICATE-----
k8s_services_cert_key: |
  ----BEGIN PRIVATE KEY----
  - Your private key here -
  -----END PRIVATE KEY-----

# A service account's credentials include a generated email address that is unique.
# Specify the email address of the user account
# You can create one according to the procedure specified in this reference
# https://developers.google.com/identity/protocols/OAuth2ServiceAccount#creatinganaccount
gce_service_account_email: '...-compute@developer.gserviceaccount.com'

# The full path of your unique service account credentials file.
# Details on generating this can be found at
# https://docs.ansible.com/ansible/guide_gce.html#credentials
# You can download json credentials according to the procedure specified in this reference
# https://support.google.com/cloud/answer/6158849?hl=en&ref_topic=6262490#serviceaccounts
gce_credentials_file: '{{ ansible_env.HOME }}/gcloud.json'

# Specify your project ID which one used from your GCP account
gce_project_id: my-project-id
